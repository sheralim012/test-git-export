0
00:00:00,000 --> 00:00:00,000


1
00:00:00,000 --> 00:00:03,000
I want to talk about more general classical error

2
00:00:03,000 --> 00:00:04,000
correcting codes.

3
00:00:04,000 --> 00:00:06,000


4
00:00:06,000 --> 00:00:09,000
And to make progress on this, what I've written down

5
00:00:09,000 --> 00:00:17,000
was extremely general, to make progress let me not

6
00:00:17,000 --> 00:00:18,000
worry about efficiency.

7
00:00:18,000 --> 00:00:21,000
So for majority-- by the way, we can do that efficiently.

8
00:00:21,000 --> 00:00:25,000
Let me not worry about that for now,

9
00:00:25,000 --> 00:00:31,000
and let me focus on this worst case error model.

10
00:00:31,000 --> 00:00:36,000
The setting in which we focus on random errors, for some reason,

11
00:00:36,000 --> 00:00:39,000
is often put under a different field.

12
00:00:39,000 --> 00:00:41,000
A field called information theory.

13
00:00:41,000 --> 00:00:44,000
Of course, they are both reasonable ways of modeling

14
00:00:44,000 --> 00:00:48,000
what happens, but the study of random errors

15
00:00:48,000 --> 00:00:51,000
will be in a different part of the class.

16
00:00:51,000 --> 00:00:54,000
So I'll talk about worst case errors, and for now,

17
00:00:54,000 --> 00:00:54,000
without efficiency.

18
00:00:54,000 --> 00:00:56,000
When we talk about fault-tolerant

19
00:00:56,000 --> 00:00:59,000
in quantum computing, we will return to efficiency.

20
00:00:59,000 --> 00:01:07,000
But for now, let me not worry about that.

21
00:01:07,000 --> 00:01:13,000
So this let's us simplify things a little bit.

22
00:01:13,000 --> 00:01:17,000
Instead of talking about the encoding and decoding maps,

23
00:01:17,000 --> 00:01:19,000
we can just talk about the set of code words.

24
00:01:19,000 --> 00:01:23,000


25
00:01:23,000 --> 00:01:25,000
So C is a set of code words, and just

26
00:01:25,000 --> 00:01:30,000
the image of the encoding map.

27
00:01:30,000 --> 00:01:42,000
So with some subset of n-bit strings with size 2 to the k.

28
00:01:42,000 --> 00:01:47,000


29
00:01:47,000 --> 00:01:58,000
So you can think of, if this is the set of all strings,

30
00:01:58,000 --> 00:02:00,000
these might be my code words.

31
00:02:00,000 --> 00:02:04,000
Now if there is some error, I flip a few bits,

32
00:02:04,000 --> 00:02:07,000
let's say I move to some nearby point here.

33
00:02:07,000 --> 00:02:12,000
The decoding map is going to be look for the nearest code word.

34
00:02:12,000 --> 00:02:14,000


35
00:02:14,000 --> 00:02:18,000
And so this is something which might be computationally

36
00:02:18,000 --> 00:02:23,000
inefficient, but it's kind of without loss of generality

37
00:02:23,000 --> 00:02:25,000
in terms of what the optimal decoder is.

38
00:02:25,000 --> 00:02:27,000
And I'll say a little bit more about that in a minute.

39
00:02:27,000 --> 00:02:40,000
So to decode x, we just output the nearest code word.

40
00:02:40,000 --> 00:02:43,000


41
00:02:43,000 --> 00:02:45,000
And so, let me say what I mean by nearest.

42
00:02:45,000 --> 00:02:49,000


43
00:02:49,000 --> 00:02:52,000
The nearest code word between the distance

44
00:02:52,000 --> 00:02:55,000
between x and y code words--

45
00:02:55,000 --> 00:02:59,000
[INAUDIBLE]

46
00:02:59,000 --> 00:03:00,000
Image.

47
00:03:00,000 --> 00:03:01,000
The image of the map.

48
00:03:01,000 --> 00:03:04,000
So the image of a function is the set of things

49
00:03:04,000 --> 00:03:05,000
that emaps do.

50
00:03:05,000 --> 00:03:06,000
Thanks.

51
00:03:06,000 --> 00:03:07,000
Yeah?

52
00:03:07,000 --> 00:03:10,000
So, why is the size of C not less than or equal to 2

53
00:03:10,000 --> 00:03:11,000
to the k?

54
00:03:11,000 --> 00:03:17,000


55
00:03:17,000 --> 00:03:21,000
Your encoding map could, in principle, map two strings

56
00:03:21,000 --> 00:03:24,000
to the same string.

57
00:03:24,000 --> 00:03:26,000
But if you did that, you would not

58
00:03:26,000 --> 00:03:31,000
be able to successfully decode because even

59
00:03:31,000 --> 00:03:33,000
if no noise happened, or whatever noise happened,

60
00:03:33,000 --> 00:03:36,000
you would not be able to unambiguously figure out

61
00:03:36,000 --> 00:03:37,000
what string you came from.

62
00:03:37,000 --> 00:03:39,000
So even though it's not part of the definition

63
00:03:39,000 --> 00:03:42,000
of an encoding map, if you demand

64
00:03:42,000 --> 00:03:44,000
that you can correct errors most of the time,

65
00:03:44,000 --> 00:03:47,000
then that means that e has to be injected.

66
00:03:47,000 --> 00:03:47,000
Yeah.

67
00:03:47,000 --> 00:03:48,000
Thanks.

68
00:03:48,000 --> 00:03:49,000
I sort of took that for granted.

69
00:03:49,000 --> 00:03:50,000
OK.

70
00:03:50,000 --> 00:03:53,000
So we talked about norms before, let me mention now

71
00:03:53,000 --> 00:03:55,000
one more distance measure.

72
00:03:55,000 --> 00:03:57,000
And the distance between two bit strings

73
00:03:57,000 --> 00:04:00,000
will just be the number of positions on which they differ.

74
00:04:00,000 --> 00:04:03,000


75
00:04:03,000 --> 00:04:06,000
So that's actually the one norm difference

76
00:04:06,000 --> 00:04:07,000
between these two vectors.

77
00:04:07,000 --> 00:04:10,000
So these are vectors of zeros and ones,

78
00:04:10,000 --> 00:04:12,000
the one norm just adds up the number

79
00:04:12,000 --> 00:04:13,000
of positions where they differ.

80
00:04:13,000 --> 00:04:22,000
This is also sometimes called the Hamming distance.

81
00:04:22,000 --> 00:04:26,000
Let me mention, by the way, I introduced the L1 and S1 norm.

82
00:04:26,000 --> 00:04:30,000
Usually people just write 1 to mean either L1 or S1,

83
00:04:30,000 --> 00:04:31,000
depending on context.

84
00:04:31,000 --> 00:04:33,000
So for vectors it means one norm,

85
00:04:33,000 --> 00:04:35,000
for matrices that means S1.

86
00:04:35,000 --> 00:04:37,000
Usually the context is clear enough

87
00:04:37,000 --> 00:04:39,000
that we can just write the number 1,

88
00:04:39,000 --> 00:04:43,000
or for p norms to write a p. et cetera.

89
00:04:43,000 --> 00:04:43,000
OK.

90
00:04:43,000 --> 00:04:49,000
So to decode, we find the code word

91
00:04:49,000 --> 00:04:52,000
that we can reach by flipping the fewest number of bits.

92
00:04:52,000 --> 00:04:55,000


93
00:04:55,000 --> 00:04:58,000
And for this worst case error model,

94
00:04:58,000 --> 00:05:03,000
this is without loss of generality.

95
00:05:03,000 --> 00:05:05,000
And what's nice about this, is now

96
00:05:05,000 --> 00:05:07,000
we don't have to specify a decoder,

97
00:05:07,000 --> 00:05:10,000
and we have a very simple way of describing the error correcting

98
00:05:10,000 --> 00:05:13,000
properties of this code.

99
00:05:13,000 --> 00:05:18,000
So the code performance--

100
00:05:18,000 --> 00:05:20,000
let me say the error correcting performance--

101
00:05:20,000 --> 00:05:30,000


102
00:05:30,000 --> 00:05:33,000
it only depends on one parameter, at least

103
00:05:33,000 --> 00:05:39,000
in the worst case, which is called the code distance.

104
00:05:39,000 --> 00:05:46,000


105
00:05:46,000 --> 00:05:50,000
We call this d, and it's defined to be

106
00:05:50,000 --> 00:05:54,000
the minimum distance between any two distinct code words.

107
00:05:54,000 --> 00:05:57,000
So the minimum distance between x

108
00:05:57,000 --> 00:06:06,000
and y, where x is not equal to y, and x and y

109
00:06:06,000 --> 00:06:09,000
are both code words.

110
00:06:09,000 --> 00:06:14,000
So you look at these code words, you check all their pairwise

111
00:06:14,000 --> 00:06:16,000
distances, whatever the closest two code words

112
00:06:16,000 --> 00:06:19,000
are, that is the distance of the code.

113
00:06:19,000 --> 00:06:22,000


114
00:06:22,000 --> 00:06:25,000
So the biggest that can be is n, right?

115
00:06:25,000 --> 00:06:28,000
Because two strings are at most distance n apart,

116
00:06:28,000 --> 00:06:30,000
and that's what we get for the repetition code.

117
00:06:30,000 --> 00:06:32,000
That's the way to correct the most errors.

118
00:06:32,000 --> 00:06:37,000
Of course, you do that at a cost of not encoding very many bits.

119
00:06:37,000 --> 00:06:40,000
But if all you care about is really protecting one bit,

120
00:06:40,000 --> 00:06:42,000
repetition code is the best way to go.

121
00:06:42,000 --> 00:06:46,000


122
00:06:46,000 --> 00:06:51,000
In fact, if you wanted a simple answer of why it's harder

123
00:06:51,000 --> 00:06:53,000
to build quantum computers than classical computers,

124
00:06:53,000 --> 00:06:56,000
I would argue that one of the central difficulties

125
00:06:56,000 --> 00:07:00,000
is that the repetition code works for classical information

126
00:07:00,000 --> 00:07:01,000
and not for quantum information.

127
00:07:01,000 --> 00:07:02,000
So we'll return to this later when

128
00:07:02,000 --> 00:07:06,000
we talk about quantum information,

129
00:07:06,000 --> 00:07:09,000
but classically, the repetition code

130
00:07:09,000 --> 00:07:12,000
is arguably what we use at the lowest level.

131
00:07:12,000 --> 00:07:16,000
In a transistor-- or maybe above a transistor,

132
00:07:16,000 --> 00:07:18,000
you have a bi-stable flip-flop, you

133
00:07:18,000 --> 00:07:21,000
have current going either one direction or another direction.

134
00:07:21,000 --> 00:07:25,000
That current is made up of millions or billions

135
00:07:25,000 --> 00:07:26,000
of electrons.

136
00:07:26,000 --> 00:07:28,000
And so that's like a giant repetition code.

137
00:07:28,000 --> 00:07:31,000
Zero is encoded in billions of electrons going one way,

138
00:07:31,000 --> 00:07:34,000
a 1 is encoded in billions of electrons going the other way.

139
00:07:34,000 --> 00:07:37,000
And the way the circuit works is it's

140
00:07:37,000 --> 00:07:39,000
measuring voltages, which are essentially

141
00:07:39,000 --> 00:07:40,000
like little majorities.

142
00:07:40,000 --> 00:07:42,000
The voltages are kind of this aggregate property of which way

143
00:07:42,000 --> 00:07:45,000
the electrons are flowing, and it's measuring,

144
00:07:45,000 --> 00:07:48,000
are the majority of electrons going this way or that way?

145
00:07:48,000 --> 00:07:50,000
Likewise, in a hard drive cell, you

146
00:07:50,000 --> 00:07:53,000
have a little bit of magnetic tape--

147
00:07:53,000 --> 00:07:55,000
like, it's now this with a solid state--

148
00:07:55,000 --> 00:07:57,000
but if you have a little bit of magnetic tape,

149
00:07:57,000 --> 00:08:00,000
there's a magnetic field which is also

150
00:08:00,000 --> 00:08:02,000
kind of the majority of which way

151
00:08:02,000 --> 00:08:03,000
the local spins are pointing.

152
00:08:03,000 --> 00:08:05,000
So at a very basic hardware level,

153
00:08:05,000 --> 00:08:09,000
our classical computers are using repetition codes

154
00:08:09,000 --> 00:08:10,000
and we take that for granted.

155
00:08:10,000 --> 00:08:12,000
If you just make the wire fatter,

156
00:08:12,000 --> 00:08:14,000
it gets more reliable, that's a basic principle

157
00:08:14,000 --> 00:08:17,000
of classical computing which goes out the window for quantum

158
00:08:17,000 --> 00:08:18,000
computing.

159
00:08:18,000 --> 00:08:23,000
Which counts for a lot of the pain in constructing these.

160
00:08:23,000 --> 00:08:26,000


161
00:08:26,000 --> 00:08:27,000
OK.

162
00:08:27,000 --> 00:08:32,000
So this code distance describes how well classical codes do.

163
00:08:32,000 --> 00:08:36,000
I've identified these three parameters-- n, k, and d.

164
00:08:36,000 --> 00:08:45,000
So we say that c is an n k, d code

165
00:08:45,000 --> 00:08:48,000
if it has the three parameters.

166
00:08:48,000 --> 00:08:51,000
And so a major opening question is, which triples

167
00:08:51,000 --> 00:08:53,000
of n, k, and d are possible?

168
00:08:53,000 --> 00:08:57,000
Even asymptotically, we don't have an exact answer for this.

169
00:08:57,000 --> 00:09:00,000


